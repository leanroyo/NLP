{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLpELbW5E-4l"
      },
      "outputs": [],
      "source": [
        "# Importaciones necesarias\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, LSTM, SimpleRNN, Embedding, Input\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import gdown\n",
        "import json\n",
        "\n",
        "# 1. Datos\n",
        "def download_dataset():\n",
        "    if not os.path.exists('data_volunteers.json'):\n",
        "        url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n",
        "        output = 'data_volunteers.json'\n",
        "        gdown.download(url, output, quiet=False)\n",
        "    else:\n",
        "        print(\"El dataset ya se encuentra descargado\")\n",
        "\n",
        "def load_dataset():\n",
        "    with open(\"data_volunteers.json\") as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt = txt.replace(\"\\'d\", \" had\").replace(\"\\'s\", \" is\").replace(\"\\'m\", \" am\").replace(\"don't\", \"do not\")\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "    return txt\n",
        "\n",
        "def prepare_data(data, max_len=30):\n",
        "    input_sentences, output_sentences, output_sentences_inputs = [], [], []\n",
        "\n",
        "    for line in data:\n",
        "        for i in range(len(line['dialog'])-1):\n",
        "            chat_in = clean_text(line['dialog'][i]['text'])\n",
        "            chat_out = clean_text(line['dialog'][i+1]['text'])\n",
        "\n",
        "            if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
        "                continue\n",
        "\n",
        "            input_sentences.append(chat_in)\n",
        "            output_sentences.append(chat_out + ' <eos>')\n",
        "            output_sentences_inputs.append('<sos> ' + chat_out)\n",
        "\n",
        "    print(\"Cantidad de rows utilizadas:\", len(input_sentences))\n",
        "    return input_sentences, output_sentences, output_sentences_inputs\n",
        "\n",
        "# 2. Preprocesamiento\n",
        "def preprocess_data(input_sentences, output_sentences, output_sentences_inputs):\n",
        "    # Tokenización de entradas\n",
        "    tokenizer_inputs = Tokenizer()\n",
        "    tokenizer_inputs.fit_on_texts(input_sentences)\n",
        "    input_sequences = tokenizer_inputs.texts_to_sequences(input_sentences)\n",
        "\n",
        "    # Tokenización de salidas\n",
        "    tokenizer_outputs = Tokenizer(filters='')\n",
        "    tokenizer_outputs.fit_on_texts(output_sentences + output_sentences_inputs)\n",
        "    output_sequences = tokenizer_outputs.texts_to_sequences(output_sentences)\n",
        "    output_sequences_inputs = tokenizer_outputs.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "    # Padding\n",
        "    encoder_input_sequences = pad_sequences(input_sequences)\n",
        "    decoder_output_sequences = pad_sequences(output_sequences)\n",
        "    decoder_input_sequences = pad_sequences(output_sequences_inputs)\n",
        "\n",
        "    # Creación de diccionarios y parámetros\n",
        "    word2idx_inputs = tokenizer_inputs.word_index\n",
        "    word2idx_outputs = tokenizer_outputs.word_index\n",
        "    max_input_len = encoder_input_sequences.shape[1]\n",
        "    max_out_len = decoder_output_sequences.shape[1]\n",
        "    num_words_output = len(word2idx_outputs) + 1\n",
        "\n",
        "    return (word2idx_inputs, max_input_len, word2idx_outputs, max_out_len, num_words_output,\n",
        "            encoder_input_sequences, decoder_output_sequences, decoder_input_sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "embeddings_index = {}\n",
        "MAX_VOCAB_SIZE = 6000\n",
        "# Descargar los embeddings desde un google drive (es la forma más rápida)\n",
        "# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n",
        "# disponibles descargar de la página oficial como se explica en el siguiente bloque de código\n",
        "import os\n",
        "import gdown\n",
        "if os.access('gloveembedding.pkl', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94&export=download'\n",
        "    output = 'gloveembedding.pkl'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"Los embeddings gloveembedding.pkl ya están descargados\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEkpop2gP3ZB",
        "outputId": "6cb45fe2-c587-40c5-a63e-f12e56896b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los embeddings gloveembedding.pkl ya están descargados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "class WordsEmbeddings(object):\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    def __init__(self):\n",
        "        # load the embeddings\n",
        "        words_embedding_pkl = Path(self.PKL_PATH)\n",
        "        if not words_embedding_pkl.is_file():\n",
        "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
        "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
        "            embeddings = self.convert_model_to_pickle()\n",
        "        else:\n",
        "            embeddings = self.load_model_from_pickle()\n",
        "        self.embeddings = embeddings\n",
        "        # build the vocabulary hashmap\n",
        "        index = np.arange(self.embeddings.shape[0])\n",
        "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
        "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
        "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
        "\n",
        "    def get_words_embeddings(self, words):\n",
        "        words_idxs = self.words2idxs(words)\n",
        "        return self.embeddings[words_idxs]['embedding']\n",
        "\n",
        "    def words2idxs(self, words):\n",
        "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
        "\n",
        "    def idxs2words(self, idxs):\n",
        "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
        "\n",
        "    def load_model_from_pickle(self):\n",
        "        self.logger.debug(\n",
        "            'loading words embeddings from pickle {}'.format(\n",
        "                self.PKL_PATH\n",
        "            )\n",
        "        )\n",
        "        max_bytes = 2**28 - 1 # 256MB\n",
        "        bytes_in = bytearray(0)\n",
        "        input_size = os.path.getsize(self.PKL_PATH)\n",
        "        with open(self.PKL_PATH, 'rb') as f_in:\n",
        "            for _ in range(0, input_size, max_bytes):\n",
        "                bytes_in += f_in.read(max_bytes)\n",
        "        embeddings = pickle.loads(bytes_in)\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "    def convert_model_to_pickle(self):\n",
        "        # create a numpy strctured array:\n",
        "        # word     embedding\n",
        "        # U50      np.float32[]\n",
        "        # word_1   a, b, c\n",
        "        # word_2   d, e, f\n",
        "        # ...\n",
        "        # word_n   g, h, i\n",
        "        self.logger.debug(\n",
        "            'converting and loading words embeddings from text file {}'.format(\n",
        "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
        "            )\n",
        "        )\n",
        "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
        "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
        "        structure = np.dtype(structure)\n",
        "        # load numpy array from disk using a generator\n",
        "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
        "            embeddings_gen = (\n",
        "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
        "                if len(line.split()[1:]) == self.N_FEATURES\n",
        "            )\n",
        "            embeddings = np.fromiter(embeddings_gen, structure)\n",
        "        # add a null embedding\n",
        "        null_embedding = np.array(\n",
        "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
        "            dtype=structure\n",
        "        )\n",
        "        embeddings = np.concatenate([embeddings, null_embedding])\n",
        "        # dump numpy array to disk using pickle\n",
        "        max_bytes = 2**28 - 1 # # 256MB\n",
        "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.PKL_PATH, 'wb') as f_out:\n",
        "            for idx in range(0, len(bytes_out), max_bytes):\n",
        "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "class GloveEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
        "    PKL_PATH = 'gloveembedding.pkl'\n",
        "    N_FEATURES = 50\n",
        "    WORD_MAX_SIZE = 60\n",
        "\n",
        "class FasttextEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
        "    PKL_PATH = 'fasttext.pkl'\n",
        "    N_FEATURES = 300\n",
        "    WORD_MAX_SIZE = 60"
      ],
      "metadata": {
        "id": "bu7DutQoQSJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. Preparar los embeddings\n",
        "def prepare_embeddings(word2idx, embedding_dim=100):\n",
        "\n",
        "    # Por una cuestion de RAM se utilizarán los embeddings de Glove de dimension 100\n",
        "    model_embeddings = GloveEmbeddings()\n",
        "\n",
        "    # Crear la Embedding matrix de las secuencias\n",
        "\n",
        "    print('preparing embedding matrix...')\n",
        "    embed_dim = model_embeddings.N_FEATURES\n",
        "    words_not_found = []\n",
        "\n",
        "    # word_index provieen del tokenizer\n",
        "\n",
        "    nb_words = min(MAX_VOCAB_SIZE, len(word2idx_inputs)) # vocab_size\n",
        "    embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "    for word, i in word2idx_inputs.items():\n",
        "        if i >= nb_words:\n",
        "            continue\n",
        "        embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n",
        "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        else:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            words_not_found.append(word)\n",
        "\n",
        "    print('number of null word embeddings:', np.sum(np.sum(embedding_matrix**2, axis=1) == 0))\n",
        "\n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "iKpDDyM1OdKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. Entrenar el modelo\n",
        "def create_model(num_encoder_tokens, num_decoder_tokens, embedding_dim, latent_dim):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(num_encoder_tokens, embedding_dim, name='encoder_embedding')\n",
        "    encoder_embedded = encoder_embedding(encoder_inputs)\n",
        "    encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedded)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(num_decoder_tokens, embedding_dim, name='decoder_embedding')\n",
        "    decoder_embedded = decoder_embedding(decoder_inputs)\n",
        "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
        "    decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(model, encoder_input_data, decoder_input_data, decoder_target_data, batch_size, epochs):\n",
        "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_split=0.2)\n",
        "\n",
        "# 5. Inferencia\n",
        "def create_inference_models(model):\n",
        "    # Encoder\n",
        "    encoder_inputs = model.input[0]  # input_1\n",
        "    encoder_embedding = model.get_layer('encoder_embedding')(encoder_inputs)\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.get_layer('encoder_lstm')(encoder_embedding)\n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = model.input[1]  # input_2\n",
        "    decoder_state_input_h = Input(shape=(model.get_layer('decoder_lstm').units,))\n",
        "    decoder_state_input_c = Input(shape=(model.get_layer('decoder_lstm').units,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_embedding = model.get_layer('decoder_embedding')(decoder_inputs)\n",
        "    decoder_outputs, state_h, state_c = model.get_layer('decoder_lstm')(\n",
        "        decoder_embedding, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = model.get_layer('decoder_dense')(decoder_outputs)\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "# Función para decodificar\n",
        "def decode_sequence(input_seq, encoder_model, decoder_model, max_decoder_seq_length, num_decoder_tokens):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = idx2word_outputs[sampled_token_index]\n",
        "        decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Exit condition: either hit max length or find stop character.\n",
        "        if (sampled_word == '<eos>' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# Ejecución principal\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Datos\n",
        "    download_dataset()\n",
        "    data = load_dataset()\n",
        "    input_sentences, output_sentences, output_sentences_inputs = prepare_data(data)\n",
        "\n",
        "    # 2. Preprocesamiento\n",
        "    (word2idx_inputs, max_input_len, word2idx_outputs, max_out_len, num_words_output,\n",
        "     encoder_input_sequences, decoder_output_sequences, decoder_input_sequences) = preprocess_data(\n",
        "        input_sentences, output_sentences, output_sentences_inputs)\n",
        "\n",
        "    # 3. Preparar los embeddings\n",
        "    embedding_dim = 100\n",
        "    embedding_matrix_inputs = prepare_embeddings(word2idx_inputs, embedding_dim)\n",
        "    embedding_matrix_outputs = prepare_embeddings(word2idx_outputs, embedding_dim)\n",
        "\n",
        "    # 4. Entrenar el modelo\n",
        "    latent_dim = 256\n",
        "    model = create_model(len(word2idx_inputs) + 1, num_words_output, embedding_dim, latent_dim)\n",
        "\n",
        "    # Convertir las secuencias de salida en one-hot encoding\n",
        "    decoder_target_data = tf.keras.utils.to_categorical(decoder_output_sequences, num_classes=num_words_output)\n",
        "\n",
        "    train_model(model, encoder_input_sequences, decoder_input_sequences, decoder_target_data,\n",
        "                batch_size=64, epochs=50)\n",
        "\n",
        "    # 5. Inferencia\n",
        "    encoder_model, decoder_model = create_inference_models(model)\n",
        "\n",
        "    # Crear diccionario inverso para las salidas\n",
        "    idx2word_outputs = {i: word for word, i in word2idx_outputs.items()}\n",
        "\n",
        "    # Ejemplo de inferencia\n",
        "    input_seq = encoder_input_sequences[0:1]\n",
        "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, max_out_len, num_words_output)\n",
        "    print('Input sentence:', input_sentences[0])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eeRLS5XOhjl",
        "outputId": "521c0474-d604-4da0-98fd-87da719759cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El dataset ya se encuentra descargado\n",
            "Cantidad de rows utilizadas: 5985\n",
            "preparing embedding matrix...\n",
            "number of null word embeddings: 38\n",
            "preparing embedding matrix...\n",
            "number of null word embeddings: 38\n",
            "Epoch 1/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 365ms/step - loss: 4.1207 - val_loss: 2.1157\n",
            "Epoch 2/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 358ms/step - loss: 2.0164 - val_loss: 1.9685\n",
            "Epoch 3/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 361ms/step - loss: 1.8138 - val_loss: 1.8402\n",
            "Epoch 4/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 359ms/step - loss: 1.6680 - val_loss: 1.7345\n",
            "Epoch 5/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 358ms/step - loss: 1.5589 - val_loss: 1.6741\n",
            "Epoch 6/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 359ms/step - loss: 1.4535 - val_loss: 1.6214\n",
            "Epoch 7/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 339ms/step - loss: 1.3657 - val_loss: 1.5997\n",
            "Epoch 8/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 353ms/step - loss: 1.3764 - val_loss: 1.5734\n",
            "Epoch 9/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 348ms/step - loss: 1.3528 - val_loss: 1.5618\n",
            "Epoch 10/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 355ms/step - loss: 1.2939 - val_loss: 1.5371\n",
            "Epoch 11/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 358ms/step - loss: 1.2891 - val_loss: 1.5208\n",
            "Epoch 12/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 358ms/step - loss: 1.2657 - val_loss: 1.5053\n",
            "Epoch 13/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 353ms/step - loss: 1.2261 - val_loss: 1.4910\n",
            "Epoch 14/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 349ms/step - loss: 1.2097 - val_loss: 1.4814\n",
            "Epoch 15/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 461ms/step - loss: 1.1651 - val_loss: 1.4697\n",
            "Epoch 16/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 361ms/step - loss: 1.1598 - val_loss: 1.4565\n",
            "Epoch 17/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 361ms/step - loss: 1.1581 - val_loss: 1.4476\n",
            "Epoch 18/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 359ms/step - loss: 1.1180 - val_loss: 1.4423\n",
            "Epoch 19/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 351ms/step - loss: 1.1237 - val_loss: 1.4391\n",
            "Epoch 20/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 356ms/step - loss: 1.1171 - val_loss: 1.4227\n",
            "Epoch 21/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 348ms/step - loss: 1.0990 - val_loss: 1.4215\n",
            "Epoch 22/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 356ms/step - loss: 1.0834 - val_loss: 1.4122\n",
            "Epoch 23/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 335ms/step - loss: 1.0840 - val_loss: 1.4072\n",
            "Epoch 24/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 347ms/step - loss: 1.0679 - val_loss: 1.4017\n",
            "Epoch 25/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 363ms/step - loss: 1.0517 - val_loss: 1.3941\n",
            "Epoch 26/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 331ms/step - loss: 1.0426 - val_loss: 1.3958\n",
            "Epoch 27/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 353ms/step - loss: 1.0244 - val_loss: 1.3889\n",
            "Epoch 28/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 351ms/step - loss: 1.0249 - val_loss: 1.3861\n",
            "Epoch 29/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 350ms/step - loss: 1.0192 - val_loss: 1.3800\n",
            "Epoch 30/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 354ms/step - loss: 0.9939 - val_loss: 1.3738\n",
            "Epoch 31/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 461ms/step - loss: 1.0075 - val_loss: 1.3725\n",
            "Epoch 32/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 364ms/step - loss: 0.9980 - val_loss: 1.3744\n",
            "Epoch 33/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 362ms/step - loss: 0.9596 - val_loss: 1.3682\n",
            "Epoch 34/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 367ms/step - loss: 0.9656 - val_loss: 1.3651\n",
            "Epoch 35/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 382ms/step - loss: 0.9521 - val_loss: 1.3648\n",
            "Epoch 36/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 365ms/step - loss: 0.9730 - val_loss: 1.3554\n",
            "Epoch 37/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 382ms/step - loss: 0.9591 - val_loss: 1.3555\n",
            "Epoch 38/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 361ms/step - loss: 0.9596 - val_loss: 1.3526\n",
            "Epoch 39/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 335ms/step - loss: 0.9280 - val_loss: 1.3525\n",
            "Epoch 40/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 347ms/step - loss: 0.9430 - val_loss: 1.3482\n",
            "Epoch 41/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 353ms/step - loss: 0.9222 - val_loss: 1.3474\n",
            "Epoch 42/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 352ms/step - loss: 0.9235 - val_loss: 1.3477\n",
            "Epoch 43/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 370ms/step - loss: 0.8958 - val_loss: 1.3467\n",
            "Epoch 44/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 350ms/step - loss: 0.9184 - val_loss: 1.3423\n",
            "Epoch 45/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 347ms/step - loss: 0.8993 - val_loss: 1.3484\n",
            "Epoch 46/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 336ms/step - loss: 0.8849 - val_loss: 1.3394\n",
            "Epoch 47/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 482ms/step - loss: 0.8922 - val_loss: 1.3430\n",
            "Epoch 48/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 337ms/step - loss: 0.8866 - val_loss: 1.3387\n",
            "Epoch 49/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 349ms/step - loss: 0.8581 - val_loss: 1.3425\n",
            "Epoch 50/50\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 361ms/step - loss: 0.8724 - val_loss: 1.3333\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Input sentence: hello \n",
            "Decoded sentence:  i am a girl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_inference(input_string, encoder_model, decoder_model, word2idx_inputs, word2idx_outputs, idx2word_outputs, max_out_len):\n",
        "    # Tokenizar y padear la entrada\n",
        "    input_seq = [word2idx_inputs.get(word, 0) for word in input_string.lower().split()]\n",
        "    input_seq = pad_sequences([input_seq], maxlen=encoder_model.input_shape[1], padding='post')\n",
        "\n",
        "    # Codificar la entrada\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generar secuencia objetivo vacía de longitud 1\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Poblar el primer carácter de la secuencia objetivo con el carácter de inicio\n",
        "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "\n",
        "    # Bucle de muestreo\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Muestrear un token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = idx2word_outputs[sampled_token_index]\n",
        "\n",
        "        # Añadir la palabra a la oración decodificada solo si no es <eos>\n",
        "        if sampled_word != '<eos>':\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Condición de salida: longitud máxima o encontrar carácter de parada\n",
        "        if (sampled_word == '<eos>' or len(decoded_sentence.split()) > max_out_len):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Actualizar la secuencia objetivo (de longitud 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Actualizar estados\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n"
      ],
      "metadata": {
        "id": "iZ_gt_YtN4pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de uso:\n",
        "input_string = \"Hello, can you help me?\"\n",
        "inference_result = generate_inference(input_string, encoder_model, decoder_model, word2idx_inputs, word2idx_outputs, idx2word_outputs, max_out_len)\n",
        "print('Input sentence:', input_string)\n",
        "print('Decoded sentence:', inference_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wwcp9hkWAZi",
        "outputId": "d8fc0995-a489-4ce0-e4a7-0d777e9b5fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Input sentence: Hello, can you help me?\n",
            "Decoded sentence: i am a vegan coach\n"
          ]
        }
      ]
    }
  ]
}