{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yeJGnCYxuF"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Modelo de lenguaje con tokenización por caracteres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5PEwGzZA9-"
      },
      "source": [
        "### Consigna\n",
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "### Sugerencias\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y-QdFbHZYj7C"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvXlEKQZdqx"
      },
      "source": [
        "### Datos\n",
        "Utilizaremos como dataset canciones de bandas de habla inglés."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# descargar de textos.info\n",
        "import urllib.request\n",
        "\n",
        "# Para leer y parsear el texto en HTML de wikipedia\n",
        "import bs4 as bs"
      ],
      "metadata": {
        "id": "7amy6uUaBLVD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "raw_html = urllib.request.urlopen('https://www.textos.info/roberto-arlt/el-crimen-casi-perfecto/ebook')\n",
        "raw_html = raw_html.read()\n",
        "\n",
        "# Parsear artículo, 'lxml' es el parser a utilizar\n",
        "article_html = BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "# Encontrar todos los párrafos del HTML (bajo los tags <p>, <div>, <section>)\n",
        "article_text = ''\n",
        "for element in article_html.find_all(['p']):\n",
        "    article_text += element.get_text() + ' '\n",
        "\n",
        "# Pasar todo el texto a minúsculas\n",
        "article_text = article_text.lower()\n",
        "\n",
        "print(article_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx8WpleJT8hS",
        "outputId": "edd3a94a-4811-499b-f496-c673aabbc2b4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " la coartada de los tres hermanos de la suicida fue verificada. ellos\r\n",
            "no habían mentido. el mayor, juan, permaneció desde las cinco de la\r\n",
            "tarde hasta las doce de la noche (la señora stevens se suicidó entre las\r\n",
            "siete y las diez de la noche) detenido en una comisaría por su\r\n",
            "participación imprudente en una accidente de tránsito. el segundo\r\n",
            "hermano, esteban, se encontraba en el pueblo de lister desde las seis de\r\n",
            "la tarde de aquel día hasta las nueve del siguiente, y, en cuanto al\r\n",
            "tercero, el doctor pablo, no se había apartado ni un momento del\r\n",
            "laboratorio de análisis de leche de la erpa cía., donde estaba adjunto a\r\n",
            "la sección de dosificación de mantecas en las cremas. lo más curioso del caso es que aquel día los tres hermanos almorzaron\r\n",
            "con la suicida para festejar su cumpleaños, y ella, a su vez, en ningún\r\n",
            "momento dejó de traslucir su intención funesta. comieron todos\r\n",
            "alegremente; luego, a las dos de la tarde, los hombres se retiraron. sus declaraciones coincidían en un todo con las de la antigua\r\n",
            "doméstica que servía hacía muchos años a la señora stevens. esta mujer,\r\n",
            "que dormía afuera del departamento, a las siete de la tarde se retiró a\r\n",
            "su casa. la última orden que recibió de la señora stevens fue que le\r\n",
            "enviara por el portero un diario de la tarde. la criada se marchó; a las\r\n",
            "siete y diez el portero le entregó a la señora stevens el diario pedido\r\n",
            "y el proceso de acción que ésta siguió antes de matarse se presume\r\n",
            "lógicamente así: la propietaria revisó las adiciones en las libretas\r\n",
            "donde llevaba anotadas las entradas y salidas de su contabilidad\r\n",
            "doméstica, porque las libretas se encontraban sobre la mesa del comedor\r\n",
            "con algunos gastos del día subrayados; luego se sirvió un vaso de agua\r\n",
            "con whisky, y en esta mezcla arrojó aproximadamente medio gramo de\r\n",
            "cianuro de potasio. a continuación se puso a leer el diario, bebió el\r\n",
            "veneno, y al sentirse morir trató de ponerse de pie y cayó sobre la\r\n",
            "alfombra. el periódico fue hallado entre sus dedos tremendamente\r\n",
            "contraídos. tal era la primera hipótesis que se desprendía del conjunto de cosas\r\n",
            "ordenadas pacíficamente en el interior del departamento pero, como se\r\n",
            "puede apreciar, este proceso de suicidio está cargado de absurdos\r\n",
            "psicológicos. ninguno de los funcionarios que intervinimos en la\r\n",
            "investigación podíamos aceptar congruentemente que la señora stevens se\r\n",
            "hubiese suicidado. sin embargo, únicamente la stevens podía haber echado\r\n",
            "el cianuro en el vaso. el whisky no contenía veneno. el agua que se\r\n",
            "agregó al whisky también era pura. podía presumirse que el veneno había\r\n",
            "sido depositado en el fondo o las paredes de la copa, pero el vaso\r\n",
            "utilizado por la suicida había sido retirado de un anaquel donde se\r\n",
            "hallaba una docena de vasos del mismo estilo; de manera que el presunto\r\n",
            "asesino no podía saber si la stevens iba a utilizar éste o aquél. la\r\n",
            "oficina policial de química nos informó que ninguno de los vasos\r\n",
            "contenía veneno adherido a sus paredes. el asunto no era fácil. las primeras pruebas, pruebas mecánicas como\r\n",
            "las llamaba yo, nos inclinaban a aceptar que la viuda se había quitado\r\n",
            "la vida por su propia mano, pero la evidencia de que ella estaba\r\n",
            "distraída leyendo un periódico cuando la sorprendió la muerte\r\n",
            "transformaba en disparatada la prueba mecánica del suicidio. tal era la situación técnica del caso cuando yo fui designado por mis\r\n",
            "superiores para continuar ocupándome de él. en cuanto a los informes de\r\n",
            "nuestro gabinete de análisis, no cabían dudas. únicamente en el vaso,\r\n",
            "donde la señora stevens había bebido, se encontraba veneno. el agua y el\r\n",
            "whisky de las botellas eran completamente inofensivos. por otra parte,\r\n",
            "la declaración del portero era terminante; nadie había visitado a la\r\n",
            "señora stevens después que él le alcanzó el periódico; de manera que si\r\n",
            "yo, después de algunas investigaciones superficiales, hubiera cerrado el\r\n",
            "sumario informando de un suicidio comprobado, mis superiores no\r\n",
            "hubiesen podido objetar palabra. sin embargo, para mí cerrar el sumario\r\n",
            "significaba confesarme fracasado. la señora stevens había sido\r\n",
            "asesinada, y había un indicio que lo comprobaba: ¿dónde se hallaba el\r\n",
            "envase que contenía el veneno antes de que ella lo arrojara en su\r\n",
            "bebida? por más que nosotros revisáramos el departamento, no nos fue posible\r\n",
            "descubrir la caja, el sobre o el frasco que contuvo el tóxico. aquel\r\n",
            "indicio resultaba extraordinariamente sugestivo. además había otro: los\r\n",
            "hermanos de la muerta eran tres bribones. los tres, en menos de diez años, habían despilfarrado los bienes que\r\n",
            "heredaron de sus padres. actualmente sus medios de vida no eran del todo\r\n",
            "satisfactorios.  juan trabajaba como ayudante de un procurador especializado en\r\n",
            "divorcios. su conducta resultó más de una vez sospechosa y lindante con\r\n",
            "la presunción de un chantaje. esteban era corredor de seguros y había\r\n",
            "asegurado a su hermana en una gruesa suma a su favor; en cuanto a pablo,\r\n",
            "trabajaba de veterinario, pero estaba descalificado por la justicia e\r\n",
            "inhabilitado para ejercer su profesión, convicto de haber dopado\r\n",
            "caballos. para no morirse de hambre ingresó en la industria lechera, se\r\n",
            "ocupaba de los análisis. tales eran los hermanos de la señora stevens. en cuanto a ésta, había\r\n",
            "enviudado tres veces. el día del “suicidio” cumplió 68 años; pero era\r\n",
            "una mujer extraordinariamente conservada, gruesa, robusta, enérgica, con\r\n",
            "el cabello totalmente renegrido. podía aspirar a casarse una cuarta vez\r\n",
            "y manejaba su casa alegremente y con puño duro. aficionada a los\r\n",
            "placeres de la mesa, su despensa estaba provista de vinos y comestibles,\r\n",
            "y no cabe duda de que sin aquel “accidente” la viuda hubiera vivido\r\n",
            "cien años. suponer que una mujer de ese carácter era capaz de\r\n",
            "suicidarse, es desconocer la naturaleza humana. su muerte beneficiaba a\r\n",
            "cada uno de los tres hermanos con doscientos treinta mil pesos. la criada de la muerta era una mujer casi estúpida, y utilizada por\r\n",
            "aquélla en las labores groseras de la casa. ahora estaba prácticamente\r\n",
            "aterrorizada al verse engranada en un procedimiento judicial. el cadáver fue descubierto por el portero y la sirvienta a las siete\r\n",
            "de la mañana, hora en que ésta, no pudiendo abrir la puerta porque las\r\n",
            "hojas estaban aseguradas por dentro con cadenas de acero, llamó en su\r\n",
            "auxilio al encargado de la casa. a las once de la mañana, como creo\r\n",
            "haber dicho anteriormente, estaban en nuestro poder los informes del\r\n",
            "laboratorio de análisis, a las tres de la tarde abandonaba yo la\r\n",
            "habitación donde quedaba detenida la sirvienta, con una idea brincando\r\n",
            "en mi imaginación: ¿y si alguien había entrado en el departamento de la\r\n",
            "viuda rompiendo un vidrio de la ventana y colocando otro después que\r\n",
            "volcó el veneno en el vaso? era una fantasía de novela policial, pero\r\n",
            "convenía verificar la hipótesis. salí decepcionado del departamento. mi conjetura era absolutamente\r\n",
            "disparatada: la masilla solidificada no revelaba mudanza alguna. eché a caminar sin prisa. el “suicidio” de la señora stevens me\r\n",
            "preocupaba (diré una enormidad) no policialmente, sino deportivamente.\r\n",
            "yo estaba en presencia de un asesino sagacísimo, posiblemente uno de los\r\n",
            "tres hermanos que había utilizado un recurso simple y complicado, pero\r\n",
            "imposible de presumir en la nitidez de aquel vacío. absorbido en mis cavilaciones, entré en un café, y tan identificado\r\n",
            "estaba en mis conjeturas, que yo, que nunca bebo bebidas alcohólicas,\r\n",
            "automáticamente pedí un whisky. ¿cuánto tiempo permaneció el whisky\r\n",
            "servido frente a mis ojos? no lo sé; pero de pronto mis ojos vieron el\r\n",
            "vaso de whisky, la garrafa de agua y un plato con trozos de hielo.\r\n",
            "atónito quedé mirando el conjunto aquel. de pronto una idea alumbró mi\r\n",
            "curiosidad, llamé al camarero, le pagué la bebida que no había tomado,\r\n",
            "subí apresuradamente a un automóvil y me dirigí a la casa de la\r\n",
            "sirvienta. una hipótesis daba grandes saltos en mi cerebro. entré en la\r\n",
            "habitación donde estaba detenida, me senté frente a ella y le dije: —míreme bien y fíjese en lo que me va a contestar: la señora stevens, ¿tomaba el whisky con hielo o sin hielo? —con hielo, señor. —¿dónde compraba el hielo? —no lo compraba, señor. en casa había una heladera pequeña que lo\r\n",
            "fabricaba en pancitos. –y la criada casi iluminada prosiguió, a pesar de\r\n",
            "su estupidez.— ahora que me acuerdo, la heladera, hasta ayer, que vino\r\n",
            "el señor pablo, estaba descompuesta. él se encargó de arreglarla en un\r\n",
            "momento. una hora después nos encontrábamos en el departamento de la suicida\r\n",
            "con el químico de nuestra oficina de análisis, el técnico retiró el agua\r\n",
            "que se encontraba en el depósito congelador de la heladera y varios\r\n",
            "pancitos de hielo. el químico inició la operación destinada a revelar la\r\n",
            "presencia del tóxico, y a los pocos minutos pudo manifestarnos: — el\r\n",
            "agua está envenenada y los panes de este hielo están fabricados con agua\r\n",
            "envenenada. nos miramos jubilosamente. el misterio estaba desentrañado. ahora era\r\n",
            "un juego reconstruir el crimen. el doctor pablo, al reparar el fusible\r\n",
            "de la heladera (defecto que localizó el técnico) arrojó en el depósito\r\n",
            "congelador una cantidad de cianuro disuelto. después, ignorante de lo\r\n",
            "que aguardaba, la señora stevens preparó un whisky; del depósito retiró\r\n",
            "un pancito de hielo (lo cual explicaba que el plato con hielo disuelto\r\n",
            "se encontrara sobre la mesa), el cual, al desleírse en el alcohol, lo\r\n",
            "envenenó poderosamente debido a su alta concentración. sin imaginarse\r\n",
            "que la muerte la aguardaba en su vicio, la señora stevens se puso a leer\r\n",
            "el periódico, hasta que juzgando el whisky suficientemente enfriado,\r\n",
            "bebió un sorbo. los efectos no se hicieron esperar. no quedaba sino ir en busca del veterinario. inútilmente lo\r\n",
            "aguardamos en su casa. ignoraban dónde se encontraba. del laboratorio\r\n",
            "donde trabajaba nos informaron que llegaría a las diez de la noche. a las once, yo, mi superior y el juez nos presentamos en el\r\n",
            "laboratorio de la erpa. el doctor pablo, en cuanto nos vio comparecer en\r\n",
            "grupo, levantó el brazo como si quisiera anatemizar nuestras\r\n",
            "investigaciones, abrió la boca y se desplomó inerte junto a la mesa de\r\n",
            "mármol. había muerto de un síncope. en su armario se encontraba un\r\n",
            "frasco de veneno. fue el asesino más ingenioso que conocí. publicado el 21 de abril de 2023 por edu robsy.\n",
            "leído 223 veces. biblioteca digital abierta, legal y gratuita para textos y libros en formato electrónico: online, pdf, epub, mobi. hotel maison carrée \n",
            "\tc/ des ramal, 48\n",
            "\t07730 alayor - menorca\n",
            "\tislas baleares\n",
            "\tespaña textos.info es un proyecto gratuito de promoción de la lectura. ayúdanos a que nuestros libros lleguen a más gente compartiendo lecturas en las redes sociales. si puedes y quieres, también puedes hacer una donación para mantener el proyecto y posibilitar que nuestra biblioteca siga creciendo. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "08G0VTzbSteH",
        "outputId": "950264c1-2c50-471f-b73b-f54d0f99936f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' la coartada de los tres hermanos de la suicida fue verificada. ellos\\r\\nno habían mentido. el mayor, juan, permaneció desde las cinco de la\\r\\ntarde hasta las doce de la noche (la señora stevens se suicidó entre las\\r\\nsiete y las diez de la noche) detenido en una comisaría por su\\r\\nparticipación imprudente en una accidente de tránsito. el segundo\\r\\nhermano, esteban, se encontraba en el pueblo de lister desde las seis de\\r\\nla tarde de aquel día hasta las nueve del siguiente, y, en cuanto al\\r\\ntercero, el doctor pablo, no se había apartado ni un momento del\\r\\nlaboratorio de análisis de leche de la erpa cía., donde estaba adjunto a\\r\\nla sección de dosificación de mantecas en las cremas. lo más curioso del caso es que aquel día los tres hermanos almorzaron\\r\\ncon la suicida para festejar su cumpleaños, y ella, a su vez, en ningún\\r\\nmomento dejó de traslucir su intención funesta. comieron todos\\r\\nalegremente; luego, a las dos de la tarde, los hombres se retiraron. sus declaraciones coincidían en un todo con las de la antigua\\r\\ndoméstica que servía hacía muchos años a la señora stevens. esta mujer,\\r\\nque dormía afuera del departamento, a las siete de la tarde se retiró a\\r\\nsu casa. la última orden que recibió de la señora stevens fue que le\\r\\nenviara por el portero un diario de la tarde. la criada se marchó; a las\\r\\nsiete y diez el portero le entregó a la señora stevens el diario pedido\\r\\ny el proceso de acción que ésta siguió antes de matarse se presume\\r\\nlógicamente así: la propietaria revisó las adiciones en las libretas\\r\\ndonde llevaba anotadas las entradas y salidas de su contabilidad\\r\\ndoméstica, porque las libretas se encontraban sobre la mesa del comedor\\r\\ncon algunos gastos del día subrayados; luego se sirvió un vaso de agua\\r\\ncon whisky, y en esta mezcla arrojó aproximadamente medio gramo de\\r\\ncianuro de potasio. a continuación se puso a leer el diario, bebió el\\r\\nveneno, y al sentirse morir trató de ponerse de pie y cayó sobre la\\r\\nalfombra. el periódico fue hallado entre sus dedos tremendamente\\r\\ncontraídos. tal era la primera hipótesis que se desprendía del conjunto de cosas\\r\\nordenadas pacíficamente en el interior del departamento pero, como se\\r\\npuede apreciar, este proceso de suicidio está cargado de absurdos\\r\\npsicológicos. ninguno de los funcionarios que intervinimos en la\\r\\ninvestigación podíamos aceptar congruentemente que la señora stevens se\\r\\nhubiese suicidado. sin embargo, únicamente la stevens podía haber echado\\r\\nel cianuro en el vaso. el whisky no contenía veneno. el agua que se\\r\\nagregó al whisky también era pura. podía presumirse que el veneno había\\r\\nsido depositado en el fondo o las paredes de la copa, pero el vaso\\r\\nutilizado por la suicida había sido retirado de un anaquel donde se\\r\\nhallaba una docena de vasos del mismo estilo; de manera que el presunto\\r\\nasesino no podía saber si la stevens iba a utilizar éste o aquél. la\\r\\noficina policial de química nos informó que ninguno de los vasos\\r\\ncontenía veneno adherido a sus paredes. el asunto no era fácil. las primeras pruebas, pruebas mecánicas como\\r\\nlas llamaba yo, nos inclinaban a aceptar que la viuda se había quitado\\r\\nla vida por su propia mano, pero la evidencia de que ella estaba\\r\\ndistraída leyendo un periódico cuando la sorprendió la muerte\\r\\ntransformaba en disparatada la prueba mecánica del suicidio. tal era la situación técnica del caso cuando yo fui designado por mis\\r\\nsuperiores para continuar ocupándome de él. en cuanto a los informes de\\r\\nnuestro gabinete de análisis, no cabían dudas. únicamente en el vaso,\\r\\ndonde la señora stevens había bebido, se encontraba veneno. el agua y el\\r\\nwhisky de las botellas eran completamente inofensivos. por otra parte,\\r\\nla declaración del portero era terminante; nadie había visitado a la\\r\\nseñora stevens después que él le alcanzó el periódico; de manera que si\\r\\nyo, después de algunas investigaciones superficiales, hubiera cerrado el\\r\\nsumario informando de un suicidio comprobado, mis superiores no\\r\\nhubiesen podido objetar palabra. sin embargo, para mí cerrar el sumario\\r\\nsignificaba confesarme fracasado. la señora stevens había sido\\r\\nasesinada, y había un indicio que lo comprobaba: ¿dónde se hallaba el\\r\\nenvase que contenía el veneno antes de que ella lo arrojara en su\\r\\nbebida? por más que nosotros revisáramos el departamento, no nos fue posible\\r\\ndescubrir la caja, el sobre o el frasco que contuvo el tóxico. aquel\\r\\nindicio resultaba extraordinariamente sugestivo. además había otro: los\\r\\nhermanos de la muerta eran tres bribones. los tres, en menos de diez años, habían despilfarrado los bienes que\\r\\nheredaron de sus padres. actualmente sus medios de vida no eran del todo\\r\\nsatisfactorios.  juan trabajaba como ayudante de un procurador especializado en\\r\\ndivorcios. su conducta resultó más de una vez sospechosa y lindante con\\r\\nla presunción de un chantaje. esteban era corredor de seguros y había\\r\\nasegurado a su hermana en una gruesa suma a su favor; en cuanto a pablo,\\r\\ntrabajaba de veterinario, pero estaba descalificado por la justicia e\\r\\ninhabilitado para ejercer su profesión, convicto de haber dopado\\r\\ncaballos. para no morirse de hambre ingresó en la industria lechera, se\\r\\nocupaba de los análisis. tales eran los hermanos de la señora stevens. en cuanto a ésta, había\\r\\nenviudado tres veces. el día del “suicidio” cumplió 68 años; pero era\\r\\nuna mujer extraordinariamente conservada, gruesa, robusta, enérgica, con\\r\\nel cabello totalmente renegrido. podía aspirar a casarse una cuarta vez\\r\\ny manejaba su casa alegremente y con puño duro. aficionada a los\\r\\nplaceres de la mesa, su despensa estaba provista de vinos y comestibles,\\r\\ny no cabe duda de que sin aquel “accidente” la viuda hubiera vivido\\r\\ncien años. suponer que una mujer de ese carácter era capaz de\\r\\nsuicidarse, es desconocer la naturaleza humana. su muerte beneficiaba a\\r\\ncada uno de los tres hermanos con doscientos treinta mil pesos. la criada de la muerta era una mujer casi estúpida, y utilizada por\\r\\naquélla en las labores groseras de la casa. ahora estaba prácticamente\\r\\naterrorizada al verse engranada en un procedimiento judicial. el cadáver fue descubierto por el portero y la sirvienta a las siete\\r\\nde la mañana, hora en que ésta, no pudiendo abrir la puerta porque las\\r\\nhojas estaban aseguradas por dentro con cadenas de acero, llamó en su\\r\\nauxilio al encargado de la casa. a las once de la mañana, como creo\\r\\nhaber dicho anteriormente, estaban en nuestro poder los informes del\\r\\nlaboratorio de análisis, a las tres de la tarde abandonaba yo la\\r\\nhabitación donde quedaba detenida la sirvienta, con una idea brincando\\r\\nen mi imaginación: ¿y si alguien había entrado en el departamento de la\\r\\nviuda rompiendo un vidrio de la ventana y colocando otro después que\\r\\nvolcó el veneno en el vaso? era una fantasía de novela policial, pero\\r\\nconvenía verificar la hipótesis. salí decepcionado del departamento. mi conjetura era absolutamente\\r\\ndisparatada: la masilla solidificada no revelaba mudanza alguna. eché a caminar sin prisa. el “suicidio” de la señora stevens me\\r\\npreocupaba (diré una enormidad) no policialmente, sino deportivamente.\\r\\nyo estaba en presencia de un asesino sagacísimo, posiblemente uno de los\\r\\ntres hermanos que había utilizado un recurso simple y complicado, pero\\r\\nimposible de presumir en la nitidez de aquel vacío. absorbido en mis cavilaciones, entré en un café, y tan identificado\\r\\nestaba en mis conjeturas, que yo, que nunca bebo bebidas alcohólicas,\\r\\nautomáticamente pedí un whisky. ¿cuánto tiempo permaneció el whisky\\r\\nservido frente a mis ojos? no lo sé; pero de pronto mis ojos vieron el\\r\\nvaso de whisky, la garrafa de agua y un plato con trozos de hielo.\\r\\natónito quedé mirando el conjunto aquel. de pronto una idea alumbró mi\\r\\ncuriosidad, llamé al camarero, le pagué la bebida que no había tomado,\\r\\nsubí apresuradamente a un automóvil y me dirigí a la casa de la\\r\\nsirvienta. una hipótesis daba grandes saltos en mi cerebro. entré en la\\r\\nhabitación donde estaba detenida, me senté frente a ella y le dije: —míreme bien y fíjese en lo que me va a contestar: la señora stevens, ¿tomaba el whisky con hielo o sin hielo? —con hielo, señor. —¿dónde compraba el hielo? —no lo compraba, señor. en casa había una heladera pequeña que lo\\r\\nfabricaba en pancitos. –y la criada casi iluminada prosiguió, a pesar de\\r\\nsu estupidez.— ahora que me acuerdo, la heladera, hasta ayer, que vino\\r\\nel señor pablo, estaba descompuesta. él se encargó de arreglarla en un\\r\\nmomento. una hora después nos encontrábamos en el departamento de la suicida\\r\\ncon el químico de nuestra oficina de análisis, el técnico retiró el agua\\r\\nque se encontraba en el depósito congelador de la heladera y varios\\r\\npancitos de hielo. el químico inició la operación destinada a revelar la\\r\\npresencia del tóxico, y a los pocos minutos pudo manifestarnos: — el\\r\\nagua está envenenada y los panes de este hielo están fabricados con agua\\r\\nenvenenada. nos miramos jubilosamente. el misterio estaba desentrañado. ahora era\\r\\nun juego reconstruir el crimen. el doctor pablo, al reparar el fusible\\r\\nde la heladera (defecto que localizó el técnico) arrojó en el depósito\\r\\ncongelador una cantidad de cianuro disuelto. después, ignorante de lo\\r\\nque aguardaba, la señora stevens preparó un whisky; del depósito retiró\\r\\nun pancito de hielo (lo cual explicaba que el plato con hielo disuelto\\r\\nse encontrara sobre la mesa), el cual, al desleírse en el alcohol, lo\\r\\nenvenenó poderosamente debido a su alta concentración. sin imaginarse\\r\\nque la muerte la aguardaba en su vicio, la señora stevens se puso a leer\\r\\nel periódico, hasta que juzgando el whisky suficientemente enfriado,\\r\\nbebió un sorbo. los efectos no se hicieron esperar. no quedaba sino ir en busca del veterinario. inútilmente lo\\r\\naguardamos en su casa. ignoraban dónde se encontraba. del laboratorio\\r\\ndonde trabajaba nos informaron que llegaría a las diez de la noche. a las once, yo, mi superior y el juez nos presentamos en el\\r\\nlaboratorio de la erpa. el doctor pablo, en cuanto nos vio comparecer en\\r\\ngrupo, levantó el brazo como si quisiera anatemizar nuestras\\r\\ninvestigaciones, abrió la boca y se desplomó inerte junto a la mesa de\\r\\nmármol. había muerto de un síncope. en su armario se encontraba un\\r\\nfrasco de veneno. fue el asesino más ingenioso que conocí. publicado el 21 de abril de 2023 por edu robsy.\\nleído 223 veces. biblioteca digital abierta, legal y gratuita para textos y libros en formato electrónico: online, pdf, epub, mobi. hotel maison carrée \\n\\tc/ des ramal, 48\\n\\t07730 alayor - menorca\\n\\tislas baleares\\n\\tespaña textos.info es un proyecto gratuito de promoción de la lectura. ayúdanos a que nuestros libros lleguen a más gente compartiendo lecturas en las redes sociales. si puedes y quieres, también puedes hacer una donación para mantener el proyecto y posibilitar que nuestra biblioteca siga creciendo. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# en article text se encuentra el texto de todo el libro\n",
        "article_text[:1000]"
      ],
      "metadata": {
        "id": "WBE0sSYuB-E6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "7a660d7d-bfc6-40ab-b11a-f27641af00c2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' la coartada de los tres hermanos de la suicida fue verificada. ellos\\r\\nno habían mentido. el mayor, juan, permaneció desde las cinco de la\\r\\ntarde hasta las doce de la noche (la señora stevens se suicidó entre las\\r\\nsiete y las diez de la noche) detenido en una comisaría por su\\r\\nparticipación imprudente en una accidente de tránsito. el segundo\\r\\nhermano, esteban, se encontraba en el pueblo de lister desde las seis de\\r\\nla tarde de aquel día hasta las nueve del siguiente, y, en cuanto al\\r\\ntercero, el doctor pablo, no se había apartado ni un momento del\\r\\nlaboratorio de análisis de leche de la erpa cía., donde estaba adjunto a\\r\\nla sección de dosificación de mantecas en las cremas. lo más curioso del caso es que aquel día los tres hermanos almorzaron\\r\\ncon la suicida para festejar su cumpleaños, y ella, a su vez, en ningún\\r\\nmomento dejó de traslucir su intención funesta. comieron todos\\r\\nalegremente; luego, a las dos de la tarde, los hombres se retiraron. sus declaraciones coincidían en un todo '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "f7SXX_MERJR4",
        "outputId": "cebfca0f-4c30-4f55-9ff8-c28b20692116"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' la coartada de los tres hermanos de la suicida fue verificada. ellos\\r\\nno habían mentido. el mayor, juan, permaneció desde las cinco de la\\r\\ntarde hasta las doce de la noche (la señora stevens se suicidó entre las\\r\\nsiete y las diez de la noche) detenido en una comisaría por su\\r\\nparticipación imprudente en una accidente de tránsito. el segundo\\r\\nhermano, esteban, se encontraba en el pueblo de lister desde las seis de\\r\\nla tarde de aquel día hasta las nueve del siguiente, y, en cuanto al\\r\\ntercero, el doctor pablo, no se había apartado ni un momento del\\r\\nlaboratorio de análisis de leche de la erpa cía., donde estaba adjunto a\\r\\nla sección de dosificación de mantecas en las cremas. lo más curioso del caso es que aquel día los tres hermanos almorzaron\\r\\ncon la suicida para festejar su cumpleaños, y ella, a su vez, en ningún\\r\\nmomento dejó de traslucir su intención funesta. comieron todos\\r\\nalegremente; luego, a las dos de la tarde, los hombres se retiraron. sus declaraciones coincidían en un todo con las de la antigua\\r\\ndoméstica que servía hacía muchos años a la señora stevens. esta mujer,\\r\\nque dormía afuera del departamento, a las siete de la tarde se retiró a\\r\\nsu casa. la última orden que recibió de la señora stevens fue que le\\r\\nenviara por el portero un diario de la tarde. la criada se marchó; a las\\r\\nsiete y diez el portero le entregó a la señora stevens el diario pedido\\r\\ny el proceso de acción que ésta siguió antes de matarse se presume\\r\\nlógicamente así: la propietaria revisó las adiciones en las libretas\\r\\ndonde llevaba anotadas las entradas y salidas de su contabilidad\\r\\ndoméstica, porque las libretas se encontraban sobre la mesa del comedor\\r\\ncon algunos gastos del día subrayados; luego se sirvió un vaso de agua\\r\\ncon whisky, y en esta mezcla arrojó aproximadamente medio gramo de\\r\\ncianuro de potasio. a continuación se puso a leer el diario, bebió el\\r\\nveneno, y al sentirse morir trató de ponerse de pie y cayó sobre la\\r\\nalfombra. el periódico fue hallado entre sus dedos tremendamente\\r\\ncontraídos. tal era la primera hipótesis que se desprendía del conjunto de cosas\\r\\nordenadas pacíficamente en el interior del departamento pero, como se\\r\\npuede apreciar, este proceso de suicidio está cargado de absurdos\\r\\npsicológicos. ninguno de los funcionarios que intervinimos en la\\r\\ninvestigación podíamos aceptar congruentemente que la señora stevens se\\r\\nhubiese suicidado. sin embargo, únicamente la stevens podía haber echado\\r\\nel cianuro en el vaso. el whisky no contenía veneno. el agua que se\\r\\nagregó al whisky también era pura. podía presumirse que el veneno había\\r\\nsido depositado en el fondo o las paredes de la copa, pero el vaso\\r\\nutilizado por la suicida había sido retirado de un anaquel donde se\\r\\nhallaba una docena de vasos del mismo estilo; de manera que el presunto\\r\\nasesino no podía saber si la stevens iba a utilizar éste o aquél. la\\r\\noficina policial de química nos informó que ninguno de los vasos\\r\\ncontenía veneno adherido a sus paredes. el asunto no era fácil. las primeras pruebas, pruebas mecánicas como\\r\\nlas llamaba yo, nos inclinaban a aceptar que la viuda se había quitado\\r\\nla vida por su propia mano, pero la evidencia de que ella estaba\\r\\ndistraída leyendo un periódico cuando la sorprendió la muerte\\r\\ntransformaba en disparatada la prueba mecánica del suicidio. tal era la situación técnica del caso cuando yo fui designado por mis\\r\\nsuperiores para continuar ocupándome de él. en cuanto a los informes de\\r\\nnuestro gabinete de análisis, no cabían dudas. únicamente en el vaso,\\r\\ndonde la señora stevens había bebido, se encontraba veneno. el agua y el\\r\\nwhisky de las botellas eran completamente inofensivos. por otra parte,\\r\\nla declaración del portero era terminante; nadie había visitado a la\\r\\nseñora stevens después que él le alcanzó el periódico; de manera que si\\r\\nyo, después de algunas investigaciones superficiales, hubiera cerrado el\\r\\nsumario informando de un suicidio comprobado, mis superiores no\\r\\nhubiesen podido objetar palabra. sin embargo, para mí cerrar el sumario\\r\\nsignificaba confesarme fracasado. la señora stevens había sido\\r\\nasesinada, y había un indicio que lo comprobaba: ¿dónde se hallaba el\\r\\nenvase que contenía el veneno antes de que ella lo arrojara en su\\r\\nbebida? por más que nosotros revisáramos el departamento, no nos fue posible\\r\\ndescubrir la caja, el sobre o el frasco que contuvo el tóxico. aquel\\r\\nindicio resultaba extraordinariamente sugestivo. además había otro: los\\r\\nhermanos de la muerta eran tres bribones. los tres, en menos de diez años, habían despilfarrado los bienes que\\r\\nheredaron de sus padres. actualmente sus medios de vida no eran del todo\\r\\nsatisfactorios.  juan trabajaba como ayudante de un procurador especializado en\\r\\ndivorcios. su conducta resultó más de una vez sospechosa y lindante con\\r\\nla presunción de un chantaje. esteban era corredor de seguros y había\\r\\nasegurado a su hermana en una gruesa suma a su favor; en cuanto a pablo,\\r\\ntrabajaba de veterinario, pero estaba descalificado por la justicia e\\r\\ninhabilitado para ejercer su profesión, convicto de haber dopado\\r\\ncaballos. para no morirse de hambre ingresó en la industria lechera, se\\r\\nocupaba de los análisis. tales eran los hermanos de la señora stevens. en cuanto a ésta, había\\r\\nenviudado tres veces. el día del “suicidio” cumplió 68 años; pero era\\r\\nuna mujer extraordinariamente conservada, gruesa, robusta, enérgica, con\\r\\nel cabello totalmente renegrido. podía aspirar a casarse una cuarta vez\\r\\ny manejaba su casa alegremente y con puño duro. aficionada a los\\r\\nplaceres de la mesa, su despensa estaba provista de vinos y comestibles,\\r\\ny no cabe duda de que sin aquel “accidente” la viuda hubiera vivido\\r\\ncien años. suponer que una mujer de ese carácter era capaz de\\r\\nsuicidarse, es desconocer la naturaleza humana. su muerte beneficiaba a\\r\\ncada uno de los tres hermanos con doscientos treinta mil pesos. la criada de la muerta era una mujer casi estúpida, y utilizada por\\r\\naquélla en las labores groseras de la casa. ahora estaba prácticamente\\r\\naterrorizada al verse engranada en un procedimiento judicial. el cadáver fue descubierto por el portero y la sirvienta a las siete\\r\\nde la mañana, hora en que ésta, no pudiendo abrir la puerta porque las\\r\\nhojas estaban aseguradas por dentro con cadenas de acero, llamó en su\\r\\nauxilio al encargado de la casa. a las once de la mañana, como creo\\r\\nhaber dicho anteriormente, estaban en nuestro poder los informes del\\r\\nlaboratorio de análisis, a las tres de la tarde abandonaba yo la\\r\\nhabitación donde quedaba detenida la sirvienta, con una idea brincando\\r\\nen mi imaginación: ¿y si alguien había entrado en el departamento de la\\r\\nviuda rompiendo un vidrio de la ventana y colocando otro después que\\r\\nvolcó el veneno en el vaso? era una fantasía de novela policial, pero\\r\\nconvenía verificar la hipótesis. salí decepcionado del departamento. mi conjetura era absolutamente\\r\\ndisparatada: la masilla solidificada no revelaba mudanza alguna. eché a caminar sin prisa. el “suicidio” de la señora stevens me\\r\\npreocupaba (diré una enormidad) no policialmente, sino deportivamente.\\r\\nyo estaba en presencia de un asesino sagacísimo, posiblemente uno de los\\r\\ntres hermanos que había utilizado un recurso simple y complicado, pero\\r\\nimposible de presumir en la nitidez de aquel vacío. absorbido en mis cavilaciones, entré en un café, y tan identificado\\r\\nestaba en mis conjeturas, que yo, que nunca bebo bebidas alcohólicas,\\r\\nautomáticamente pedí un whisky. ¿cuánto tiempo permaneció el whisky\\r\\nservido frente a mis ojos? no lo sé; pero de pronto mis ojos vieron el\\r\\nvaso de whisky, la garrafa de agua y un plato con trozos de hielo.\\r\\natónito quedé mirando el conjunto aquel. de pronto una idea alumbró mi\\r\\ncuriosidad, llamé al camarero, le pagué la bebida que no había tomado,\\r\\nsubí apresuradamente a un automóvil y me dirigí a la casa de la\\r\\nsirvienta. una hipótesis daba grandes saltos en mi cerebro. entré en la\\r\\nhabitación donde estaba detenida, me senté frente a ella y le dije: —míreme bien y fíjese en lo que me va a contestar: la señora stevens, ¿tomaba el whisky con hielo o sin hielo? —con hielo, señor. —¿dónde compraba el hielo? —no lo compraba, señor. en casa había una heladera pequeña que lo\\r\\nfabricaba en pancitos. –y la criada casi iluminada prosiguió, a pesar de\\r\\nsu estupidez.— ahora que me acuerdo, la heladera, hasta ayer, que vino\\r\\nel señor pablo, estaba descompuesta. él se encargó de arreglarla en un\\r\\nmomento. una hora después nos encontrábamos en el departamento de la suicida\\r\\ncon el químico de nuestra oficina de análisis, el técnico retiró el agua\\r\\nque se encontraba en el depósito congelador de la heladera y varios\\r\\npancitos de hielo. el químico inició la operación destinada a revelar la\\r\\npresencia del tóxico, y a los pocos minutos pudo manifestarnos: — el\\r\\nagua está envenenada y los panes de este hielo están fabricados con agua\\r\\nenvenenada. nos miramos jubilosamente. el misterio estaba desentrañado. ahora era\\r\\nun juego reconstruir el crimen. el doctor pablo, al reparar el fusible\\r\\nde la heladera (defecto que localizó el técnico) arrojó en el depósito\\r\\ncongelador una cantidad de cianuro disuelto. después, ignorante de lo\\r\\nque aguardaba, la señora stevens preparó un whisky; del depósito retiró\\r\\nun pancito de hielo (lo cual explicaba que el plato con hielo disuelto\\r\\nse encontrara sobre la mesa), el cual, al desleírse en el alcohol, lo\\r\\nenvenenó poderosamente debido a su alta concentración. sin imaginarse\\r\\nque la muerte la aguardaba en su vicio, la señora stevens se puso a leer\\r\\nel periódico, hasta que juzgando el whisky suficientemente enfriado,\\r\\nbebió un sorbo. los efectos no se hicieron esperar. no quedaba sino ir en busca del veterinario. inútilmente lo\\r\\naguardamos en su casa. ignoraban dónde se encontraba. del laboratorio\\r\\ndonde trabajaba nos informaron que llegaría a las diez de la noche. a las once, yo, mi superior y el juez nos presentamos en el\\r\\nlaboratorio de la erpa. el doctor pablo, en cuanto nos vio comparecer en\\r\\ngrupo, levantó el brazo como si quisiera anatemizar nuestras\\r\\ninvestigaciones, abrió la boca y se desplomó inerte junto a la mesa de\\r\\nmármol. había muerto de un síncope. en su armario se encontraba un\\r\\nfrasco de veneno. fue el asesino más ingenioso que conocí. publicado el 21 de abril de 2023 por edu robsy.\\nleído 223 veces. biblioteca digital abierta, legal y gratuita para textos y libros en formato electrónico: online, pdf, epub, mobi. hotel maison carrée \\n\\tc/ des ramal, 48\\n\\t07730 alayor - menorca\\n\\tislas baleares\\n\\tespaña textos.info es un proyecto gratuito de promoción de la lectura. ayúdanos a que nuestros libros lleguen a más gente compartiendo lecturas en las redes sociales. si puedes y quieres, también puedes hacer una donación para mantener el proyecto y posibilitar que nuestra biblioteca siga creciendo. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP1JdiOIKQWi"
      },
      "source": [
        "### Elegir el tamaño del contexto\n",
        "\n",
        "En este caso, como el modelo de lenguaje es por caracteres, todo un gran corpus\n",
        "de texto puede ser considerado un documento en sí mismo y el tamaño de contexto\n",
        "puede ser elegido con más libertad en comparación a un modelo de lenguaje tokenizado por palabras y dividido en documentos más acotados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "wumBNwdjJM3j"
      },
      "outputs": [],
      "source": [
        "# seleccionamos el tamaño de contexto\n",
        "max_context_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "m5FeTaGvbDbw"
      },
      "outputs": [],
      "source": [
        "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
        "from tensorflow.keras.utils import pad_sequences # se utilizará para padding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# en este caso el vocabulario es el conjunto único de caracteres que existe en todo el texto\n",
        "chars_vocab = set(article_text)"
      ],
      "metadata": {
        "id": "573Cg5n7VhWw"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# la longitud de vocabulario de caracteres es:\n",
        "len(chars_vocab)"
      ],
      "metadata": {
        "id": "VwTK6xgLJd8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497ed3e5-78be-4ce1-e63e-a9f5026d3798"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construimos los dicionarios que asignan índices a caracteres y viceversa.\n",
        "# El diccionario `char2idx` servirá como tokenizador.\n",
        "char2idx = {k: v for v,k in enumerate(chars_vocab)}\n",
        "idx2char = {v: k for k,v in char2idx.items()}"
      ],
      "metadata": {
        "id": "2W0AeQjXV1Ou"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oIUjVU0LB0r"
      },
      "source": [
        "###  Tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizamos el texto completo\n",
        "tokenized_text = [char2idx[ch] for ch in article_text]"
      ],
      "metadata": {
        "id": "h07G3srdJppo"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text[:2000]"
      ],
      "metadata": {
        "id": "PwGVSKOiJ5bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6cb02a-b42c-4e16-c5ad-b8e88bbefee6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50,\n",
              " 55,\n",
              " 56,\n",
              " 50,\n",
              " 6,\n",
              " 14,\n",
              " 56,\n",
              " 16,\n",
              " 47,\n",
              " 56,\n",
              " 8,\n",
              " 56,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 14,\n",
              " 2,\n",
              " 50,\n",
              " 47,\n",
              " 16,\n",
              " 31,\n",
              " 2,\n",
              " 50,\n",
              " 43,\n",
              " 31,\n",
              " 16,\n",
              " 33,\n",
              " 56,\n",
              " 10,\n",
              " 14,\n",
              " 2,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 50,\n",
              " 2,\n",
              " 35,\n",
              " 29,\n",
              " 6,\n",
              " 29,\n",
              " 8,\n",
              " 56,\n",
              " 50,\n",
              " 54,\n",
              " 35,\n",
              " 31,\n",
              " 50,\n",
              " 49,\n",
              " 31,\n",
              " 16,\n",
              " 29,\n",
              " 54,\n",
              " 29,\n",
              " 6,\n",
              " 56,\n",
              " 8,\n",
              " 56,\n",
              " 36,\n",
              " 50,\n",
              " 31,\n",
              " 55,\n",
              " 55,\n",
              " 14,\n",
              " 2,\n",
              " 3,\n",
              " 34,\n",
              " 10,\n",
              " 14,\n",
              " 50,\n",
              " 43,\n",
              " 56,\n",
              " 57,\n",
              " 46,\n",
              " 56,\n",
              " 10,\n",
              " 50,\n",
              " 33,\n",
              " 31,\n",
              " 10,\n",
              " 47,\n",
              " 29,\n",
              " 8,\n",
              " 14,\n",
              " 36,\n",
              " 50,\n",
              " 31,\n",
              " 55,\n",
              " 50,\n",
              " 33,\n",
              " 56,\n",
              " 51,\n",
              " 14,\n",
              " 16,\n",
              " 24,\n",
              " 50,\n",
              " 9,\n",
              " 35,\n",
              " 56,\n",
              " 10,\n",
              " 24,\n",
              " 50,\n",
              " 4,\n",
              " 31,\n",
              " 16,\n",
              " 33,\n",
              " 56,\n",
              " 10,\n",
              " 31,\n",
              " 6,\n",
              " 29,\n",
              " 27,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 2,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 2,\n",
              " 50,\n",
              " 6,\n",
              " 29,\n",
              " 10,\n",
              " 6,\n",
              " 14,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 3,\n",
              " 34,\n",
              " 47,\n",
              " 56,\n",
              " 16,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 43,\n",
              " 56,\n",
              " 2,\n",
              " 47,\n",
              " 56,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 2,\n",
              " 50,\n",
              " 8,\n",
              " 14,\n",
              " 6,\n",
              " 31,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 50,\n",
              " 10,\n",
              " 14,\n",
              " 6,\n",
              " 43,\n",
              " 31,\n",
              " 50,\n",
              " 52,\n",
              " 55,\n",
              " 56,\n",
              " 50,\n",
              " 2,\n",
              " 31,\n",
              " 38,\n",
              " 14,\n",
              " 16,\n",
              " 56,\n",
              " 50,\n",
              " 2,\n",
              " 47,\n",
              " 31,\n",
              " 49,\n",
              " 31,\n",
              " 10,\n",
              " 2,\n",
              " 50,\n",
              " 2,\n",
              " 31,\n",
              " 50,\n",
              " 2,\n",
              " 35,\n",
              " 29,\n",
              " 6,\n",
              " 29,\n",
              " 8,\n",
              " 27,\n",
              " 50,\n",
              " 31,\n",
              " 10,\n",
              " 47,\n",
              " 16,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 2,\n",
              " 3,\n",
              " 34,\n",
              " 2,\n",
              " 29,\n",
              " 31,\n",
              " 47,\n",
              " 31,\n",
              " 50,\n",
              " 51,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 2,\n",
              " 50,\n",
              " 8,\n",
              " 29,\n",
              " 31,\n",
              " 15,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 50,\n",
              " 10,\n",
              " 14,\n",
              " 6,\n",
              " 43,\n",
              " 31,\n",
              " 40,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 47,\n",
              " 31,\n",
              " 10,\n",
              " 29,\n",
              " 8,\n",
              " 14,\n",
              " 50,\n",
              " 31,\n",
              " 10,\n",
              " 50,\n",
              " 35,\n",
              " 10,\n",
              " 56,\n",
              " 50,\n",
              " 6,\n",
              " 14,\n",
              " 33,\n",
              " 29,\n",
              " 2,\n",
              " 56,\n",
              " 16,\n",
              " 46,\n",
              " 56,\n",
              " 50,\n",
              " 4,\n",
              " 14,\n",
              " 16,\n",
              " 50,\n",
              " 2,\n",
              " 35,\n",
              " 3,\n",
              " 34,\n",
              " 4,\n",
              " 56,\n",
              " 16,\n",
              " 47,\n",
              " 29,\n",
              " 6,\n",
              " 29,\n",
              " 4,\n",
              " 56,\n",
              " 6,\n",
              " 29,\n",
              " 27,\n",
              " 10,\n",
              " 50,\n",
              " 29,\n",
              " 33,\n",
              " 4,\n",
              " 16,\n",
              " 35,\n",
              " 8,\n",
              " 31,\n",
              " 10,\n",
              " 47,\n",
              " 31,\n",
              " 50,\n",
              " 31,\n",
              " 10,\n",
              " 50,\n",
              " 35,\n",
              " 10,\n",
              " 56,\n",
              " 50,\n",
              " 56,\n",
              " 6,\n",
              " 6,\n",
              " 29,\n",
              " 8,\n",
              " 31,\n",
              " 10,\n",
              " 47,\n",
              " 31,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 47,\n",
              " 16,\n",
              " 11,\n",
              " 10,\n",
              " 2,\n",
              " 29,\n",
              " 47,\n",
              " 14,\n",
              " 36,\n",
              " 50,\n",
              " 31,\n",
              " 55,\n",
              " 50,\n",
              " 2,\n",
              " 31,\n",
              " 19,\n",
              " 35,\n",
              " 10,\n",
              " 8,\n",
              " 14,\n",
              " 3,\n",
              " 34,\n",
              " 43,\n",
              " 31,\n",
              " 16,\n",
              " 33,\n",
              " 56,\n",
              " 10,\n",
              " 14,\n",
              " 24,\n",
              " 50,\n",
              " 31,\n",
              " 2,\n",
              " 47,\n",
              " 31,\n",
              " 57,\n",
              " 56,\n",
              " 10,\n",
              " 24,\n",
              " 50,\n",
              " 2,\n",
              " 31,\n",
              " 50,\n",
              " 31,\n",
              " 10,\n",
              " 6,\n",
              " 14,\n",
              " 10,\n",
              " 47,\n",
              " 16,\n",
              " 56,\n",
              " 57,\n",
              " 56,\n",
              " 50,\n",
              " 31,\n",
              " 10,\n",
              " 50,\n",
              " 31,\n",
              " 55,\n",
              " 50,\n",
              " 4,\n",
              " 35,\n",
              " 31,\n",
              " 57,\n",
              " 55,\n",
              " 14,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 29,\n",
              " 2,\n",
              " 47,\n",
              " 31,\n",
              " 16,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 2,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 2,\n",
              " 50,\n",
              " 2,\n",
              " 31,\n",
              " 29,\n",
              " 2,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 3,\n",
              " 34,\n",
              " 55,\n",
              " 56,\n",
              " 50,\n",
              " 47,\n",
              " 56,\n",
              " 16,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 56,\n",
              " 28,\n",
              " 35,\n",
              " 31,\n",
              " 55,\n",
              " 50,\n",
              " 8,\n",
              " 46,\n",
              " 56,\n",
              " 50,\n",
              " 43,\n",
              " 56,\n",
              " 2,\n",
              " 47,\n",
              " 56,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 2,\n",
              " 50,\n",
              " 10,\n",
              " 35,\n",
              " 31,\n",
              " 49,\n",
              " 31,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 55,\n",
              " 50,\n",
              " 2,\n",
              " 29,\n",
              " 19,\n",
              " 35,\n",
              " 29,\n",
              " 31,\n",
              " 10,\n",
              " 47,\n",
              " 31,\n",
              " 24,\n",
              " 50,\n",
              " 51,\n",
              " 24,\n",
              " 50,\n",
              " 31,\n",
              " 10,\n",
              " 50,\n",
              " 6,\n",
              " 35,\n",
              " 56,\n",
              " 10,\n",
              " 47,\n",
              " 14,\n",
              " 50,\n",
              " 56,\n",
              " 55,\n",
              " 3,\n",
              " 34,\n",
              " 47,\n",
              " 31,\n",
              " 16,\n",
              " 6,\n",
              " 31,\n",
              " 16,\n",
              " 14,\n",
              " 24,\n",
              " 50,\n",
              " 31,\n",
              " 55,\n",
              " 50,\n",
              " 8,\n",
              " 14,\n",
              " 6,\n",
              " 47,\n",
              " 14,\n",
              " 16,\n",
              " 50,\n",
              " 4,\n",
              " 56,\n",
              " 57,\n",
              " 55,\n",
              " 14,\n",
              " 24,\n",
              " 50,\n",
              " 10,\n",
              " 14,\n",
              " 50,\n",
              " 2,\n",
              " 31,\n",
              " 50,\n",
              " 43,\n",
              " 56,\n",
              " 57,\n",
              " 46,\n",
              " 56,\n",
              " 50,\n",
              " 56,\n",
              " 4,\n",
              " 56,\n",
              " 16,\n",
              " 47,\n",
              " 56,\n",
              " 8,\n",
              " 14,\n",
              " 50,\n",
              " 10,\n",
              " 29,\n",
              " 50,\n",
              " 35,\n",
              " 10,\n",
              " 50,\n",
              " 33,\n",
              " 14,\n",
              " 33,\n",
              " 31,\n",
              " 10,\n",
              " 47,\n",
              " 14,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 55,\n",
              " 3,\n",
              " 34,\n",
              " 55,\n",
              " 56,\n",
              " 57,\n",
              " 14,\n",
              " 16,\n",
              " 56,\n",
              " 47,\n",
              " 14,\n",
              " 16,\n",
              " 29,\n",
              " 14,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 56,\n",
              " 10,\n",
              " 11,\n",
              " 55,\n",
              " 29,\n",
              " 2,\n",
              " 29,\n",
              " 2,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 31,\n",
              " 6,\n",
              " 43,\n",
              " 31,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 50,\n",
              " 31,\n",
              " 16,\n",
              " 4,\n",
              " 56,\n",
              " 50,\n",
              " 6,\n",
              " 46,\n",
              " 56,\n",
              " 36,\n",
              " 24,\n",
              " 50,\n",
              " 8,\n",
              " 14,\n",
              " 10,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 31,\n",
              " 2,\n",
              " 47,\n",
              " 56,\n",
              " 57,\n",
              " 56,\n",
              " 50,\n",
              " 56,\n",
              " 8,\n",
              " 9,\n",
              " 35,\n",
              " 10,\n",
              " 47,\n",
              " 14,\n",
              " 50,\n",
              " 56,\n",
              " 3,\n",
              " 34,\n",
              " 55,\n",
              " 56,\n",
              " 50,\n",
              " 2,\n",
              " 31,\n",
              " 6,\n",
              " 6,\n",
              " 29,\n",
              " 27,\n",
              " 10,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 8,\n",
              " 14,\n",
              " 2,\n",
              " 29,\n",
              " 54,\n",
              " 29,\n",
              " 6,\n",
              " 56,\n",
              " 6,\n",
              " 29,\n",
              " 27,\n",
              " 10,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 33,\n",
              " 56,\n",
              " 10,\n",
              " 47,\n",
              " 31,\n",
              " 6,\n",
              " 56,\n",
              " 2,\n",
              " 50,\n",
              " 31,\n",
              " 10,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 2,\n",
              " 50,\n",
              " 6,\n",
              " 16,\n",
              " 31,\n",
              " 33,\n",
              " 56,\n",
              " 2,\n",
              " 36,\n",
              " 50,\n",
              " 55,\n",
              " 14,\n",
              " 50,\n",
              " 33,\n",
              " 11,\n",
              " 2,\n",
              " 50,\n",
              " 6,\n",
              " 35,\n",
              " 16,\n",
              " 29,\n",
              " 14,\n",
              " 2,\n",
              " 14,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 55,\n",
              " 50,\n",
              " 6,\n",
              " 56,\n",
              " 2,\n",
              " 14,\n",
              " 50,\n",
              " 31,\n",
              " 2,\n",
              " 50,\n",
              " 28,\n",
              " 35,\n",
              " 31,\n",
              " 50,\n",
              " 56,\n",
              " 28,\n",
              " 35,\n",
              " 31,\n",
              " 55,\n",
              " 50,\n",
              " 8,\n",
              " 46,\n",
              " 56,\n",
              " 50,\n",
              " 55,\n",
              " 14,\n",
              " 2,\n",
              " 50,\n",
              " 47,\n",
              " 16,\n",
              " 31,\n",
              " 2,\n",
              " 50,\n",
              " 43,\n",
              " 31,\n",
              " 16,\n",
              " 33,\n",
              " 56,\n",
              " 10,\n",
              " 14,\n",
              " 2,\n",
              " 50,\n",
              " 56,\n",
              " 55,\n",
              " 33,\n",
              " 14,\n",
              " 16,\n",
              " 15,\n",
              " 56,\n",
              " 16,\n",
              " 14,\n",
              " 10,\n",
              " 3,\n",
              " 34,\n",
              " 6,\n",
              " 14,\n",
              " 10,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 50,\n",
              " 2,\n",
              " 35,\n",
              " 29,\n",
              " 6,\n",
              " 29,\n",
              " 8,\n",
              " 56,\n",
              " 50,\n",
              " 4,\n",
              " 56,\n",
              " 16,\n",
              " 56,\n",
              " 50,\n",
              " 54,\n",
              " 31,\n",
              " 2,\n",
              " 47,\n",
              " 31,\n",
              " 9,\n",
              " 56,\n",
              " 16,\n",
              " 50,\n",
              " 2,\n",
              " 35,\n",
              " 50,\n",
              " 6,\n",
              " 35,\n",
              " 33,\n",
              " 4,\n",
              " 55,\n",
              " 31,\n",
              " 56,\n",
              " 38,\n",
              " 14,\n",
              " 2,\n",
              " 24,\n",
              " 50,\n",
              " 51,\n",
              " 50,\n",
              " 31,\n",
              " 55,\n",
              " 55,\n",
              " 56,\n",
              " 24,\n",
              " 50,\n",
              " 56,\n",
              " 50,\n",
              " 2,\n",
              " 35,\n",
              " 50,\n",
              " 49,\n",
              " 31,\n",
              " 15,\n",
              " 24,\n",
              " 50,\n",
              " 31,\n",
              " 10,\n",
              " 50,\n",
              " 10,\n",
              " 29,\n",
              " 10,\n",
              " 19,\n",
              " 21,\n",
              " 10,\n",
              " 3,\n",
              " 34,\n",
              " 33,\n",
              " 14,\n",
              " 33,\n",
              " 31,\n",
              " 10,\n",
              " 47,\n",
              " 14,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 9,\n",
              " 27,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 47,\n",
              " 16,\n",
              " 56,\n",
              " 2,\n",
              " 55,\n",
              " 35,\n",
              " 6,\n",
              " 29,\n",
              " 16,\n",
              " 50,\n",
              " 2,\n",
              " 35,\n",
              " 50,\n",
              " 29,\n",
              " 10,\n",
              " 47,\n",
              " 31,\n",
              " 10,\n",
              " 6,\n",
              " 29,\n",
              " 27,\n",
              " 10,\n",
              " 50,\n",
              " 54,\n",
              " 35,\n",
              " 10,\n",
              " 31,\n",
              " 2,\n",
              " 47,\n",
              " 56,\n",
              " 36,\n",
              " 50,\n",
              " 6,\n",
              " 14,\n",
              " 33,\n",
              " 29,\n",
              " 31,\n",
              " 16,\n",
              " 14,\n",
              " 10,\n",
              " 50,\n",
              " 47,\n",
              " 14,\n",
              " 8,\n",
              " 14,\n",
              " 2,\n",
              " 3,\n",
              " 34,\n",
              " 56,\n",
              " 55,\n",
              " 31,\n",
              " 19,\n",
              " 16,\n",
              " 31,\n",
              " 33,\n",
              " 31,\n",
              " 10,\n",
              " 47,\n",
              " 31,\n",
              " 41,\n",
              " 50,\n",
              " 55,\n",
              " 35,\n",
              " 31,\n",
              " 19,\n",
              " 14,\n",
              " 24,\n",
              " 50,\n",
              " 56,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 2,\n",
              " 50,\n",
              " 8,\n",
              " 14,\n",
              " 2,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 50,\n",
              " 55,\n",
              " 56,\n",
              " 50,\n",
              " 47,\n",
              " 56,\n",
              " 16,\n",
              " 8,\n",
              " 31,\n",
              " 24,\n",
              " 50,\n",
              " 55,\n",
              " 14,\n",
              " 2,\n",
              " 50,\n",
              " 43,\n",
              " 14,\n",
              " 33,\n",
              " 57,\n",
              " 16,\n",
              " 31,\n",
              " 2,\n",
              " 50,\n",
              " 2,\n",
              " 31,\n",
              " 50,\n",
              " 16,\n",
              " 31,\n",
              " 47,\n",
              " 29,\n",
              " 16,\n",
              " 56,\n",
              " 16,\n",
              " 14,\n",
              " 10,\n",
              " 36,\n",
              " 50,\n",
              " 2,\n",
              " 35,\n",
              " 2,\n",
              " 50,\n",
              " 8,\n",
              " 31,\n",
              " 6,\n",
              " 55,\n",
              " 56,\n",
              " 16,\n",
              " 56,\n",
              " 6,\n",
              " 29,\n",
              " 14,\n",
              " 10,\n",
              " 31,\n",
              " 2,\n",
              " 50,\n",
              " 6,\n",
              " 14,\n",
              " 29,\n",
              " 10,\n",
              " 6,\n",
              " 29,\n",
              " 8,\n",
              " 46,\n",
              " 56,\n",
              " 10,\n",
              " 50,\n",
              " 31,\n",
              " 10,\n",
              " 50,\n",
              " 35,\n",
              " 10,\n",
              " 50,\n",
              " 47,\n",
              " 14,\n",
              " 8,\n",
              " 14,\n",
              " 50,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpYcaypKcI9"
      },
      "source": [
        "### Organizando y estructurando el dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# separaremos el dataset entre entrenamiento y validación.\n",
        "# `p_val` será la proporción del corpus que se reservará para validación\n",
        "# `num_val` es la cantidad de secuencias de tamaño `max_context_size` que se usará en validación\n",
        "p_val = 0.1\n",
        "num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))"
      ],
      "metadata": {
        "id": "WSSmg9jtKP0T"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separamos la porción de texto utilizada en entrenamiento de la de validación.\n",
        "train_text = tokenized_text[:-num_val*max_context_size]\n",
        "val_text = tokenized_text[-num_val*max_context_size:]"
      ],
      "metadata": {
        "id": "b7dCpGrdKll0"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences_val = [val_text[init*max_context_size:init*(max_context_size+1)] for init in range(num_val)]"
      ],
      "metadata": {
        "id": "NmxQdxl8LRCg"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences_train = [train_text[init:init+max_context_size] for init in range(len(train_text)-max_context_size+1)]"
      ],
      "metadata": {
        "id": "_gyFT9koLqDm"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(tokenized_sentences_train[:-1])\n",
        "y = np.array(tokenized_sentences_train[1:])"
      ],
      "metadata": {
        "id": "oVNqmmLRodT0"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nótese que estamos estructurando el problema de aprendizaje como *many-to-many*:\n",
        "\n",
        "Entrada: secuencia de tokens [$x_0$, $x_1$, ..., $x_N$]\n",
        "\n",
        "Target: secuencia de tokens [$x_1$, $x_2$, ..., $x_{N+1}$]\n",
        "\n",
        "De manera que la red tiene que aprender que su salida deben ser los tokens desplazados en una posición y un nuevo token predicho (el N+1).\n",
        "\n",
        "La ventaja de estructurar el aprendizaje de esta manera es que para cada token de target se propaga una señal de gradiente por el grafo de cómputo recurrente, que es mejor que estructurar el problema como *many-to-one* en donde sólo una señal de gradiente se propaga."
      ],
      "metadata": {
        "id": "Vken7O4ETsAJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iPTx-UJl6r"
      },
      "source": [
        "En este punto tenemos en la variable `tokenized_sentences` los versos tokenizados. Vamos a quedarnos con un conjunto de validación que utilizaremos para medir la calidad de la generación de secuencias con la métrica de Perplejidad."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "KFAyA4zCWE-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe622d7e-6667-4109-fabb-45b2b804f787"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9585, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0,:10]"
      ],
      "metadata": {
        "id": "qcKRl70HFTzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c906db2c-27ea-47e9-8518-325b9c6ba341"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([50, 55, 56, 50,  6, 14, 56, 16, 47, 56])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0,:10]"
      ],
      "metadata": {
        "id": "TVpLCKSZFXZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e94058-7b3e-4a90-ecee-15c13d664b86"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([55, 56, 50,  6, 14, 56, 16, 47, 56,  8])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars_vocab)"
      ],
      "metadata": {
        "id": "wOFCR-KqbW1N"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definiendo el modelo"
      ],
      "metadata": {
        "id": "tnnjdAQ5UAEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, TimeDistributed, CategoryEncoding, SimpleRNN, Dense\n",
        "from keras.models import Model, Sequential"
      ],
      "metadata": {
        "id": "rkMCZvmhrQz4"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo que se propone como ejemplo consume los índices de los tokens y los transforma en vectores OHE (en este caso no entrenamos una capa de embedding para caracteres). Esa transformación se logra combinando las capas `CategoryEncoding` que transforma a índices a vectores OHE y `TimeDistributed` que aplica la capa a lo largo de la dimensión \"temporal\" de la secuencia."
      ],
      "metadata": {
        "id": "wgz7VKwTUbj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(TimeDistributed(CategoryEncoding(num_tokens=vocab_size, output_mode = \"one_hot\"),input_shape=(None,1)))\n",
        "model.add(SimpleRNN(200, return_sequences=True, dropout=0.1, recurrent_dropout=0.1 ))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Zd2OkfQYs2Q7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "a06892f6-75c6-442e-fd92-d79118856f53"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed_1 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)           │          \u001b[38;5;34m51,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)            │          \u001b[38;5;34m11,658\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">51,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">11,658</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m63,458\u001b[0m (247.88 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,458</span> (247.88 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m63,458\u001b[0m (247.88 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,458</span> (247.88 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmJWNyxQwfCE"
      },
      "source": [
        "\n",
        "### Definir el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWK3z85sQfUe"
      },
      "source": [
        "Dado que por el momento no hay implementaciones adecuadas de la perplejidad que puedan operar en tiempo de entrenamiento, armaremos un Callback *ad-hoc* que la calcule en cada epoch.\n",
        "\n",
        "**Nota**: un Callback es una rutina gatillada por algún evento, son muy útiles para relevar datos en diferentes momentos del desarrollo del modelo. En este caso queremos hacer un cálculo cada vez que termina una epoch de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "zUHX3r5JD-MG"
      },
      "outputs": [],
      "source": [
        "class PplCallback(keras.callbacks.Callback):\n",
        "\n",
        "    '''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
        "    si la perplejidad no mejora después de `patience` epochs.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data, history_ppl,patience=5):\n",
        "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "      # mediremos la perplejidad\n",
        "      self.val_data = val_data\n",
        "\n",
        "      self.target = []\n",
        "      self.padded = []\n",
        "\n",
        "      count = 0\n",
        "      self.info = []\n",
        "      self.min_score = np.inf\n",
        "      self.patience_counter = 0\n",
        "      self.patience = patience\n",
        "\n",
        "      # nos movemos en todas las secuencias de los datos de validación\n",
        "      for seq in self.val_data:\n",
        "\n",
        "        len_seq = len(seq)\n",
        "        # armamos todas las subsecuencias\n",
        "        subseq = [seq[:i] for i in range(1,len_seq)]\n",
        "        self.target.extend([seq[i] for i in range(1,len_seq)])\n",
        "\n",
        "        if len(subseq)!=0:\n",
        "\n",
        "          self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "\n",
        "          self.info.append((count,count+len_seq))\n",
        "          count += len_seq\n",
        "\n",
        "      self.padded = np.vstack(self.padded)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        "        scores = []\n",
        "\n",
        "        predictions = self.model.predict(self.padded,verbose=0)\n",
        "\n",
        "        # para cada secuencia de validación\n",
        "        for start,end in self.info:\n",
        "\n",
        "          # en `probs` iremos guardando las probabilidades de los términos target\n",
        "          probs = [predictions[idx_seq,-1,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
        "\n",
        "          # calculamos la perplejidad por medio de logaritmos\n",
        "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        current_score = np.mean(scores)\n",
        "        history_ppl.append(current_score)\n",
        "        print(f'\\n mean perplexity: {current_score} \\n')\n",
        "\n",
        "        # chequeamos si tenemos que detener el entrenamiento\n",
        "        if current_score < self.min_score:\n",
        "          self.min_score = current_score\n",
        "          self.model.save(\"my_model.keras\")\n",
        "          print(\"Saved new model!\")\n",
        "          self.patience_counter = 0\n",
        "        else:\n",
        "          self.patience_counter += 1\n",
        "          if self.patience_counter == self.patience:\n",
        "            print(\"Stopping training...\")\n",
        "            self.model.stop_training = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBZIwR0gruA"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "oQq1PHDkxDvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d161dc-5b10-4a55-98d1-5ee5b9085b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - loss: 3.2967\n",
            " mean perplexity: 16.257381009886508 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 443ms/step - loss: 3.2910\n",
            "Epoch 2/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step - loss: 2.8135\n",
            " mean perplexity: 13.734731929023289 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 347ms/step - loss: 2.8117\n",
            "Epoch 3/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - loss: 2.5402\n",
            " mean perplexity: 12.365352591218697 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 354ms/step - loss: 2.5388\n",
            "Epoch 4/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step - loss: 2.3315\n",
            " mean perplexity: 12.189931040951803 \n",
            "\n",
            "Saved new model!\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 366ms/step - loss: 2.3306\n",
            "Epoch 5/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step - loss: 2.1951\n",
            " mean perplexity: 14.046080596896498 \n",
            "\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 381ms/step - loss: 2.1944\n",
            "Epoch 6/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - loss: 2.0983\n",
            " mean perplexity: 14.36541032762296 \n",
            "\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 348ms/step - loss: 2.0979\n",
            "Epoch 7/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - loss: 2.0376\n",
            " mean perplexity: 16.21343828817479 \n",
            "\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 353ms/step - loss: 2.0373\n",
            "Epoch 8/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step - loss: 1.9829\n",
            " mean perplexity: 17.865482201538722 \n",
            "\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 359ms/step - loss: 1.9827\n",
            "Epoch 9/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - loss: 1.9453\n",
            " mean perplexity: 16.22859291882459 \n",
            "\n",
            "Stopping training...\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 342ms/step - loss: 1.9451\n"
          ]
        }
      ],
      "source": [
        "# fiteamos, nótese el agregado del callback con su inicialización. El batch_size lo podemos seleccionar a mano\n",
        "# en general, lo mejor es escoger el batch más grande posible que minimice el tiempo de cada época.\n",
        "# En la variable `history_ppl` se guardarán los valores de perplejidad para cada época.\n",
        "history_ppl = []\n",
        "hist = model.fit(X, y, epochs=20, callbacks=[PplCallback(tokenized_sentences_val,history_ppl)], batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(history_ppl) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history_ppl)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K30JHB3Dv-mx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "34a2c1f3-433d-45d5-821f-b3d524d5df96"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN6Fg_BsxJe6"
      },
      "source": [
        "\n",
        "### Predicción del próximo caracter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "IBvKHFPmzpy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d51487-b0bf-4aaf-bfc5-a4dc4292ce07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Se puede usar gradio para probar el modelo\n",
        "# Gradio es una herramienta muy útil para crear interfaces para ensayar modelos\n",
        "# https://gradio.app/\n",
        "\n",
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "HNyBykvhzs7-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "outputId": "9ff36520-f5e2-4deb-c800-76247f511c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://9002823f0948a0612b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9002823f0948a0612b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9002823f0948a0612b.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def model_response(human_text):\n",
        "\n",
        "    # Encodeamos\n",
        "    encoded = [char2idx[ch] for ch in human_text.lower() ]\n",
        "    # Si tienen distinto largo\n",
        "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
        "\n",
        "    # Predicción softmax\n",
        "    y_hat = np.argmax(model.predict(encoded)[0,-1,:])\n",
        "\n",
        "\n",
        "    # Debemos buscar en el vocabulario el caracter\n",
        "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "    out_word = ''\n",
        "    out_word = idx2char[y_hat]\n",
        "\n",
        "    # Agrego la palabra a la frase predicha\n",
        "    return human_text + out_word\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCeMWWupxN1-"
      },
      "source": [
        "### Generación de secuencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "bwbS_pfhxvB3"
      },
      "outputs": [],
      "source": [
        "def generate_seq(model, seed_text, max_length, n_words):\n",
        "    \"\"\"\n",
        "        Exec model sequence prediction\n",
        "\n",
        "        Args:\n",
        "            model (keras): modelo entrenado\n",
        "            seed_text (string): texto de entrada (input_seq)\n",
        "            max_length (int): máxima longitud de la sequencia de entrada\n",
        "            n_words (int): números de caracteres a agregar a la sequencia de entrada\n",
        "        returns:\n",
        "            output_text (string): sentencia con las \"n_words\" agregadas\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "\t\t# Encodeamos\n",
        "        encoded = [char2idx[ch] for ch in output_text.lower() ]\n",
        "\t\t# Si tienen distinto largo\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "\t\t# Predicción softmax\n",
        "        y_hat = np.argmax(model.predict(encoded,verbose=0)[0,-1,:])\n",
        "\t\t# Vamos concatenando las predicciones\n",
        "        out_word = ''\n",
        "\n",
        "        out_word = idx2char[y_hat]\n",
        "\n",
        "\t\t# Agrego las palabras a la frase predicha\n",
        "        output_text += out_word\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "JoFqRC5pxzqS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bf10b259-382e-4ae8-8855-b50096ef0391"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yo fui el porieno de la señora stevens s'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "input_text='yo fui el '\n",
        "\n",
        "generate_seq(model, input_text, max_length=max_context_size, n_words=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drJ6xn5qW1Hl"
      },
      "source": [
        "###  Beam search y muestreo aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "_vovn9XZW1Hl"
      },
      "outputs": [],
      "source": [
        "# funcionalidades para hacer encoding y decoding\n",
        "\n",
        "def encode(text,max_length=max_context_size):\n",
        "\n",
        "    encoded = [char2idx[ch] for ch in text]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def decode(seq):\n",
        "    return ''.join([idx2char[ch] for ch in seq])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "I_lZiQwkW1Hl"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "# función que selecciona candidatos para el beam search\n",
        "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp,mode):\n",
        "\n",
        "  # colectar todas las probabilidades para la siguiente búsqueda\n",
        "  pred_large = []\n",
        "\n",
        "  for idx,pp in enumerate(pred):\n",
        "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
        "\n",
        "  pred_large = np.array(pred_large)\n",
        "\n",
        "  # criterio de selección\n",
        "  if mode == 'det':\n",
        "    idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
        "  elif mode == 'sto':\n",
        "    idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo aleatorio\n",
        "  else:\n",
        "    raise ValueError(f'Wrong selection mode. {mode} was given. det and sto are supported.')\n",
        "\n",
        "  # traducir a índices de token en el vocabulario\n",
        "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
        "                        np.array([idx_select%vocab_size]).T),\n",
        "                      axis=1)\n",
        "\n",
        "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
        "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
        "\n",
        "\n",
        "def beam_search(model,num_beams,num_words,input,temp=2,mode='det'):\n",
        "\n",
        "    # first iteration\n",
        "\n",
        "    # encode\n",
        "    encoded = encode(input)\n",
        "\n",
        "    # first prediction\n",
        "    y_hat = model.predict(encoded,verbose=0)[0,-1,:]\n",
        "\n",
        "    # get vocabulary size\n",
        "    vocab_size = y_hat.shape[0]\n",
        "\n",
        "    # initialize history\n",
        "    history_probs = [0]*num_beams\n",
        "    history_tokens = [encoded[0]]*num_beams\n",
        "\n",
        "    # select num_beams candidates\n",
        "    history_probs, history_tokens = select_candidates([y_hat],\n",
        "                                        num_beams,\n",
        "                                        vocab_size,\n",
        "                                        history_probs,\n",
        "                                        history_tokens,\n",
        "                                        temp,\n",
        "                                        mode)\n",
        "\n",
        "    # beam search loop\n",
        "    for i in range(num_words-1):\n",
        "\n",
        "      preds = []\n",
        "\n",
        "      for hist in history_tokens:\n",
        "\n",
        "        # actualizar secuencia de tokens\n",
        "        input_update = np.array([hist[i+1:]]).copy()\n",
        "\n",
        "        # predicción\n",
        "        y_hat = model.predict(input_update,verbose=0)[0,-1,:]\n",
        "\n",
        "        preds.append(y_hat)\n",
        "\n",
        "      history_probs, history_tokens = select_candidates(preds,\n",
        "                                                        num_beams,\n",
        "                                                        vocab_size,\n",
        "                                                        history_probs,\n",
        "                                                        history_tokens,\n",
        "                                                        temp,\n",
        "                                                        mode)\n",
        "\n",
        "    return history_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "GeLqAoOYW1Hm"
      },
      "outputs": [],
      "source": [
        "# predicción con beam search\n",
        "salidas = beam_search(model,num_beams=10,num_words=25,input=\"yo soy \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salidas[0]"
      ],
      "metadata": {
        "id": "P8HQoLhw-NYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c1e797-96e3-4c8c-e3c2-f23d0c6736ca"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0, 51, 14, 50,  2, 14, 51, 50, 55, 56,\n",
              "       50,  2, 31, 38, 14, 16, 56, 50,  2, 47, 31, 49, 31, 10,  2, 50,  8,\n",
              "       31, 50, 55, 56, 50,  2])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "2S3_I3S1W1Hm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cecf19a-52bc-40b4-9075-76dc39212f2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxyo soy la señora stevens de la s'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "# veamos las salidas\n",
        "decode(salidas[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_LlqmtEW1Hn"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}